{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/youngdd4/qwen/blob/main/DeepSeek_R1_Distill_Llama_8B_GGUF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNkpBtLvuTOp"
      },
      "source": [
        "## DeepSeek-R1-Distill-Llama-8B-GGUF WebUI\n",
        "\n",
        "1. Run the following cell, takes ~5 min\n",
        "(You may need to confirm to proceed by typing \"Y\")\n",
        "2. Click the gradio link at the bottom\n",
        "3. Template/Prompt format:\n",
        "```\n",
        "<｜begin▁of▁sentence｜>{system_prompt}<｜User｜>{prompt}<｜Assistant｜><｜end▁of▁sentence｜><｜Assistant｜>\n",
        "```\n",
        "\n",
        "Tip: As thinking models need a lot more tokens for reasoning, adjust max_new_tokens to a higher value in Prameter settings.\n",
        "\n",
        "Quantized model: https://huggingface.co/bartowski/DeepSeek-R1-Distill-Llama-8B-GGUF\n",
        "\n",
        "Original model: https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B\n",
        "\n",
        "Want to try other local LLMs? Check out this repo: https://github.com/Troyanovsky/Local-LLM-Comparison-Colab-UI/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-MiHp_bveP6",
        "outputId": "29b5d69b-70e3-4fae-b505-5f68e83d95f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Requirement already satisfied: torch==2.2.2 in /usr/local/lib/python3.11/dist-packages (2.2.2+cu121)\n",
            "Requirement already satisfied: torchvision==0.17.2 in /usr/local/lib/python3.11/dist-packages (0.17.2+cu121)\n",
            "Requirement already satisfied: torchaudio==2.2.2 in /usr/local/lib/python3.11/dist-packages (2.2.2+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (4.13.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (2.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.17.2) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.17.2) (10.4.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.2) (12.5.82)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.2.2) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.2.2) (1.3.0)\n",
            "fatal: destination path 'text-generation-webui' already exists and is not an empty directory.\n",
            "/content/text-generation-webui\n",
            "Collecting llama-cpp-python==0.3.6+cpuavx2 (from -r requirements.txt (line 35))\n",
            "  Downloading https://github.com/oobabooga/llama-cpp-python-cuBLAS-wheels/releases/download/cpu/llama_cpp_python-0.3.6+cpuavx2-cp311-cp311-linux_x86_64.whl (4.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hIgnoring llama-cpp-python: markers 'platform_system == \"Linux\" and platform_machine == \"x86_64\" and python_version == \"3.10\"' don't match your environment\n",
            "Ignoring llama-cpp-python: markers 'platform_system == \"Windows\" and python_version == \"3.11\"' don't match your environment\n",
            "Ignoring llama-cpp-python: markers 'platform_system == \"Windows\" and python_version == \"3.10\"' don't match your environment\n",
            "Ignoring llama-cpp-python-cuda: markers 'platform_system == \"Windows\" and python_version == \"3.11\"' don't match your environment\n",
            "Ignoring llama-cpp-python-cuda: markers 'platform_system == \"Windows\" and python_version == \"3.10\"' don't match your environment\n",
            "Collecting llama-cpp-python-cuda==0.3.6+cu121 (from -r requirements.txt (line 43))\n",
            "  Downloading https://github.com/oobabooga/llama-cpp-python-cuBLAS-wheels/releases/download/textgen-webui/llama_cpp_python_cuda-0.3.6+cu121-cp311-cp311-linux_x86_64.whl (450.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m450.4/450.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hIgnoring llama-cpp-python-cuda: markers 'platform_system == \"Linux\" and platform_machine == \"x86_64\" and python_version == \"3.10\"' don't match your environment\n",
            "Ignoring llama-cpp-python-cuda-tensorcores: markers 'platform_system == \"Windows\" and python_version == \"3.11\"' don't match your environment\n",
            "Ignoring llama-cpp-python-cuda-tensorcores: markers 'platform_system == \"Windows\" and python_version == \"3.10\"' don't match your environment\n",
            "Collecting llama-cpp-python-cuda-tensorcores==0.3.6+cu121 (from -r requirements.txt (line 49))\n",
            "  Downloading https://github.com/oobabooga/llama-cpp-python-cuBLAS-wheels/releases/download/textgen-webui/llama_cpp_python_cuda_tensorcores-0.3.6+cu121-cp311-cp311-linux_x86_64.whl (487.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m487.1/487.1 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hIgnoring llama-cpp-python-cuda-tensorcores: markers 'platform_system == \"Linux\" and platform_machine == \"x86_64\" and python_version == \"3.10\"' don't match your environment\n",
            "Ignoring exllamav2: markers 'platform_system == \"Windows\" and python_version == \"3.11\"' don't match your environment\n",
            "Ignoring exllamav2: markers 'platform_system == \"Windows\" and python_version == \"3.10\"' don't match your environment\n",
            "Collecting exllamav2==0.2.7+cu121.torch2.4.1 (from -r requirements.txt (line 55))\n",
            "  Downloading https://github.com/oobabooga/exllamav2/releases/download/v0.2.7/exllamav2-0.2.7+cu121.torch2.4.1-cp311-cp311-linux_x86_64.whl (137.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.1/137.1 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hIgnoring exllamav2: markers 'platform_system == \"Linux\" and platform_machine == \"x86_64\" and python_version == \"3.10\"' don't match your environment\n",
            "Ignoring exllamav2: markers 'platform_system == \"Linux\" and platform_machine != \"x86_64\"' don't match your environment\n",
            "Ignoring flash-attn: markers 'platform_system == \"Windows\" and python_version == \"3.11\"' don't match your environment\n",
            "Ignoring flash-attn: markers 'platform_system == \"Windows\" and python_version == \"3.10\"' don't match your environment\n",
            "Collecting flash-attn==2.7.3+cu12torch2.4cxx11abiFALSE (from -r requirements.txt (line 60))\n",
            "  Downloading https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.3/flash_attn-2.7.3+cu12torch2.4cxx11abiFALSE-cp311-cp311-linux_x86_64.whl (191.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m191.4/191.4 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hIgnoring flash-attn: markers 'platform_system == \"Linux\" and platform_machine == \"x86_64\" and python_version == \"3.10\"' don't match your environment\n",
            "Requirement already satisfied: accelerate==1.2.* in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (1.2.1)\n",
            "Requirement already satisfied: bitsandbytes==0.45.* in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (0.45.5)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (0.4.6)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (2.14.4)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (0.8.1)\n",
            "Requirement already satisfied: fastapi==0.112.4 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (0.112.4)\n",
            "Requirement already satisfied: gradio==4.37.* in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (4.37.2)\n",
            "Requirement already satisfied: jinja2==3.1.5 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (3.1.5)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 9)) (3.8)\n",
            "Requirement already satisfied: numba==0.59.* in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 10)) (0.59.1)\n",
            "Requirement already satisfied: numpy==1.26.* in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 11)) (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 12)) (2.2.2)\n",
            "Requirement already satisfied: peft==0.12.* in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 13)) (0.12.0)\n",
            "Requirement already satisfied: Pillow>=9.5.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 14)) (10.4.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 15)) (5.9.5)\n",
            "Requirement already satisfied: pydantic==2.8.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 16)) (2.8.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 17)) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 18)) (2.32.3)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 19)) (13.9.4)\n",
            "Requirement already satisfied: safetensors==0.5.* in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 20)) (0.5.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 21)) (1.15.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 22)) (0.2.0)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 23)) (2.18.0)\n",
            "Requirement already satisfied: transformers==4.48.* in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 24)) (4.48.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 25)) (4.67.1)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 26)) (0.19.11)\n",
            "Requirement already satisfied: SpeechRecognition==3.10.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 29)) (3.10.0)\n",
            "Requirement already satisfied: flask_cloudflared==0.0.14 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 30)) (0.0.14)\n",
            "Requirement already satisfied: sse-starlette==1.6.5 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 31)) (1.6.5)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 32)) (0.9.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate==1.2.*->-r requirements.txt (line 1)) (24.2)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from accelerate==1.2.*->-r requirements.txt (line 1)) (2.2.2+cu121)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate==1.2.*->-r requirements.txt (line 1)) (0.31.4)\n",
            "Requirement already satisfied: starlette<0.39.0,>=0.37.2 in /usr/local/lib/python3.11/dist-packages (from fastapi==0.112.4->-r requirements.txt (line 6)) (0.38.6)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi==0.112.4->-r requirements.txt (line 6)) (4.13.2)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.37.*->-r requirements.txt (line 7)) (23.2.1)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.37.*->-r requirements.txt (line 7)) (5.5.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio==4.37.*->-r requirements.txt (line 7)) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.0.2 in /usr/local/lib/python3.11/dist-packages (from gradio==4.37.*->-r requirements.txt (line 7)) (1.0.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio==4.37.*->-r requirements.txt (line 7)) (0.28.1)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.11/dist-packages (from gradio==4.37.*->-r requirements.txt (line 7)) (6.5.2)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.37.*->-r requirements.txt (line 7)) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.37.*->-r requirements.txt (line 7)) (3.10.0)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.37.*->-r requirements.txt (line 7)) (3.10.18)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio==4.37.*->-r requirements.txt (line 7)) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.11/dist-packages (from gradio==4.37.*->-r requirements.txt (line 7)) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.11/dist-packages (from gradio==4.37.*->-r requirements.txt (line 7)) (0.11.12)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.37.*->-r requirements.txt (line 7)) (2.10.0)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.37.*->-r requirements.txt (line 7)) (0.12.0)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio==4.37.*->-r requirements.txt (line 7)) (0.15.3)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.37.*->-r requirements.txt (line 7)) (2.4.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.37.*->-r requirements.txt (line 7)) (0.34.2)\n",
            "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba==0.59.*->-r requirements.txt (line 10)) (0.42.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic==2.8.2->-r requirements.txt (line 16)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.11/dist-packages (from pydantic==2.8.2->-r requirements.txt (line 16)) (2.20.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.48.*->-r requirements.txt (line 24)) (3.18.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.48.*->-r requirements.txt (line 24)) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.48.*->-r requirements.txt (line 24)) (0.21.1)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.11/dist-packages (from flask_cloudflared==0.0.14->-r requirements.txt (line 30)) (3.1.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.0.2->gradio==4.37.*->-r requirements.txt (line 7)) (2025.3.2)\n",
            "Requirement already satisfied: websockets<12.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.0.2->gradio==4.37.*->-r requirements.txt (line 7)) (11.0.3)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->-r requirements.txt (line 4)) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets->-r requirements.txt (line 4)) (0.3.7)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets->-r requirements.txt (line 4)) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets->-r requirements.txt (line 4)) (0.70.15)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets->-r requirements.txt (line 4)) (3.11.15)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->-r requirements.txt (line 12)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->-r requirements.txt (line 12)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->-r requirements.txt (line 12)) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->-r requirements.txt (line 18)) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->-r requirements.txt (line 18)) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->-r requirements.txt (line 18)) (2025.4.26)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->-r requirements.txt (line 19)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->-r requirements.txt (line 19)) (2.19.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 23)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 23)) (1.71.0)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 23)) (5.29.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 23)) (75.2.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 23)) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 23)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 23)) (3.1.3)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 26)) (8.2.1)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 26)) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 26)) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 26)) (4.3.8)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 26)) (2.29.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 26)) (1.3.6)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python==0.3.6+cpuavx2->-r requirements.txt (line 35)) (5.6.3)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (from exllamav2==0.2.7+cu121.torch2.4.1->-r requirements.txt (line 55)) (1.11.1.4)\n",
            "Requirement already satisfied: fastparquet in /usr/local/lib/python3.11/dist-packages (from exllamav2==0.2.7+cu121.torch2.4.1->-r requirements.txt (line 55)) (2024.11.0)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6.0,>=4.2.0->gradio==4.37.*->-r requirements.txt (line 7)) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6.0,>=4.2.0->gradio==4.37.*->-r requirements.txt (line 7)) (1.40.0)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from Flask>=0.8->flask_cloudflared==0.0.14->-r requirements.txt (line 30)) (1.9.0)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from Flask>=0.8->flask_cloudflared==0.0.14->-r requirements.txt (line 30)) (2.2.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->-r requirements.txt (line 4)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->-r requirements.txt (line 4)) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->-r requirements.txt (line 4)) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->-r requirements.txt (line 4)) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->-r requirements.txt (line 4)) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->-r requirements.txt (line 4)) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->-r requirements.txt (line 4)) (1.20.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 26)) (4.0.12)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio==4.37.*->-r requirements.txt (line 7)) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio==4.37.*->-r requirements.txt (line 7)) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio==4.37.*->-r requirements.txt (line 7)) (0.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->-r requirements.txt (line 19)) (0.1.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->gradio==4.37.*->-r requirements.txt (line 7)) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->gradio==4.37.*->-r requirements.txt (line 7)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->gradio==4.37.*->-r requirements.txt (line 7)) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->gradio==4.37.*->-r requirements.txt (line 7)) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->gradio==4.37.*->-r requirements.txt (line 7)) (3.2.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==1.2.*->-r requirements.txt (line 1)) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==1.2.*->-r requirements.txt (line 1)) (3.4.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==1.2.*->-r requirements.txt (line 1)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==1.2.*->-r requirements.txt (line 1)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==1.2.*->-r requirements.txt (line 1)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==1.2.*->-r requirements.txt (line 1)) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==1.2.*->-r requirements.txt (line 1)) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==1.2.*->-r requirements.txt (line 1)) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==1.2.*->-r requirements.txt (line 1)) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==1.2.*->-r requirements.txt (line 1)) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==1.2.*->-r requirements.txt (line 1)) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==1.2.*->-r requirements.txt (line 1)) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==1.2.*->-r requirements.txt (line 1)) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==1.2.*->-r requirements.txt (line 1)) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate==1.2.*->-r requirements.txt (line 1)) (12.5.82)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio==4.37.*->-r requirements.txt (line 7)) (1.5.4)\n",
            "Requirement already satisfied: cramjam>=2.3 in /usr/local/lib/python3.11/dist-packages (from fastparquet->exllamav2==0.2.7+cu121.torch2.4.1->-r requirements.txt (line 55)) (2.10.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.24.1->gradio==4.37.*->-r requirements.txt (line 7)) (1.3.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 26)) (5.0.2)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.37.*->-r requirements.txt (line 7)) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.37.*->-r requirements.txt (line 7)) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.37.*->-r requirements.txt (line 7)) (0.25.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=1.10.0->accelerate==1.2.*->-r requirements.txt (line 1)) (1.3.0)\n",
            "\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "41ae43|\u001b[1;32mOK\u001b[0m  |       0B/s|/content/text-generation-webui/models//DeepSeek-R1-Distill-Llama-8B-Q5_K_M.gguf\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n",
            "/content/text-generation-webui\n",
            "\u001b[2;36m17:07:13-853034\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Starting Text generation web UI                        \n",
            "\u001b[2;36m17:07:14-178597\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading \u001b[32m\"DeepSeek-R1-Distill-Llama-8B-Q5_K_M.gguf\"\u001b[0m     \n",
            "\u001b[2;36m17:07:14-667608\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m llama.cpp weights detected:                            \n",
            "\u001b[2;36m                \u001b[0m         \u001b[32m\"models/DeepSeek-R1-Distill-Llama-8B-Q5_K_M.gguf\"\u001b[0m      \n",
            "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    yes\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "llama_model_load_from_file: using device CUDA0 (Tesla T4) - 14992 MiB free\n",
            "llama_model_loader: loaded meta data with 32 key-value pairs and 292 tensors from models/DeepSeek-R1-Distill-Llama-8B-Q5_K_M.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Llama 8B\n",
            "llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Llama\n",
            "llama_model_loader: - kv   4:                         general.size_label str              = 8B\n",
            "llama_model_loader: - kv   5:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   6:                       llama.context_length u32              = 131072\n",
            "llama_model_loader: - kv   7:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   8:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   9:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv  10:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  12:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  13:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  19:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 128001\n",
            "llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 128001\n",
            "llama_model_loader: - kv  23:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  24:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\n",
            "llama_model_loader: - kv  26:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  27:                          general.file_type u32              = 17\n",
            "llama_model_loader: - kv  28:                      quantize.imatrix.file str              = /models_out/DeepSeek-R1-Distill-Llama...\n",
            "llama_model_loader: - kv  29:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\n",
            "llama_model_loader: - kv  30:             quantize.imatrix.entries_count i32              = 224\n",
            "llama_model_loader: - kv  31:              quantize.imatrix.chunks_count i32              = 125\n",
            "llama_model_loader: - type  f32:   66 tensors\n",
            "llama_model_loader: - type q5_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: control token: 128254 '<|reserved_special_token_246|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128249 '<|reserved_special_token_241|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128246 '<|reserved_special_token_238|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128243 '<|reserved_special_token_235|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128242 '<|reserved_special_token_234|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128241 '<|reserved_special_token_233|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128240 '<|reserved_special_token_232|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128235 '<|reserved_special_token_227|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128231 '<|reserved_special_token_223|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128230 '<|reserved_special_token_222|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128228 '<|reserved_special_token_220|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128225 '<|reserved_special_token_217|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128218 '<|reserved_special_token_210|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128214 '<|reserved_special_token_206|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128213 '<|reserved_special_token_205|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128207 '<|reserved_special_token_199|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128206 '<|reserved_special_token_198|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128204 '<|reserved_special_token_196|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128200 '<|reserved_special_token_192|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128199 '<|reserved_special_token_191|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128198 '<|reserved_special_token_190|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128196 '<|reserved_special_token_188|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128194 '<|reserved_special_token_186|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128193 '<|reserved_special_token_185|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128188 '<|reserved_special_token_180|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128187 '<|reserved_special_token_179|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128185 '<|reserved_special_token_177|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128184 '<|reserved_special_token_176|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128180 '<|reserved_special_token_172|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128179 '<|reserved_special_token_171|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128178 '<|reserved_special_token_170|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128177 '<|reserved_special_token_169|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128176 '<|reserved_special_token_168|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128175 '<|reserved_special_token_167|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128171 '<|reserved_special_token_163|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128170 '<|reserved_special_token_162|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128169 '<|reserved_special_token_161|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128168 '<|reserved_special_token_160|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128165 '<|reserved_special_token_157|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128162 '<|reserved_special_token_154|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128158 '<|reserved_special_token_150|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128156 '<|reserved_special_token_148|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128155 '<|reserved_special_token_147|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128154 '<|reserved_special_token_146|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128151 '<|reserved_special_token_143|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128149 '<|reserved_special_token_141|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128147 '<|reserved_special_token_139|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128146 '<|reserved_special_token_138|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128144 '<|reserved_special_token_136|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128142 '<|reserved_special_token_134|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128141 '<|reserved_special_token_133|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128138 '<|reserved_special_token_130|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128136 '<|reserved_special_token_128|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128135 '<|reserved_special_token_127|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128134 '<|reserved_special_token_126|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128133 '<|reserved_special_token_125|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128131 '<|reserved_special_token_123|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128128 '<|reserved_special_token_120|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128124 '<|reserved_special_token_116|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128123 '<|reserved_special_token_115|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128122 '<|reserved_special_token_114|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128119 '<|reserved_special_token_111|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128115 '<|reserved_special_token_107|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128112 '<|reserved_special_token_104|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128110 '<|reserved_special_token_102|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128109 '<|reserved_special_token_101|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128108 '<|reserved_special_token_100|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128106 '<|reserved_special_token_98|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128103 '<|reserved_special_token_95|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128102 '<|reserved_special_token_94|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128101 '<|reserved_special_token_93|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128097 '<|reserved_special_token_89|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128091 '<|reserved_special_token_83|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128090 '<|reserved_special_token_82|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128089 '<|reserved_special_token_81|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128087 '<|reserved_special_token_79|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128085 '<|reserved_special_token_77|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128081 '<|reserved_special_token_73|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128078 '<|reserved_special_token_70|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128076 '<|reserved_special_token_68|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128075 '<|reserved_special_token_67|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128073 '<|reserved_special_token_65|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128068 '<|reserved_special_token_60|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128067 '<|reserved_special_token_59|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128065 '<|reserved_special_token_57|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128063 '<|reserved_special_token_55|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128062 '<|reserved_special_token_54|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128060 '<|reserved_special_token_52|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128059 '<|reserved_special_token_51|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128057 '<|reserved_special_token_49|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128054 '<|reserved_special_token_46|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128046 '<|reserved_special_token_38|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128045 '<|reserved_special_token_37|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128044 '<|reserved_special_token_36|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128043 '<|reserved_special_token_35|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128038 '<|reserved_special_token_30|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128036 '<|reserved_special_token_28|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128035 '<|reserved_special_token_27|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128032 '<|reserved_special_token_24|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128028 '<|reserved_special_token_20|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128027 '<|reserved_special_token_19|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128024 '<|reserved_special_token_16|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128023 '<|reserved_special_token_15|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128022 '<|reserved_special_token_14|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128021 '<|reserved_special_token_13|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128018 '<|reserved_special_token_10|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128016 '<|reserved_special_token_8|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128012 '<｜Assistant｜>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128011 '<｜User｜>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128005 '<|reserved_special_token_2|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128004 '<|finetune_right_pad_id|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128002 '<|reserved_special_token_0|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128252 '<|reserved_special_token_244|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128190 '<|reserved_special_token_182|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128183 '<|reserved_special_token_175|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128137 '<|reserved_special_token_129|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128182 '<|reserved_special_token_174|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128040 '<|reserved_special_token_32|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128048 '<|reserved_special_token_40|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128092 '<|reserved_special_token_84|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128215 '<|reserved_special_token_207|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128107 '<|reserved_special_token_99|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128208 '<|reserved_special_token_200|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128145 '<|reserved_special_token_137|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128031 '<|reserved_special_token_23|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128129 '<|reserved_special_token_121|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128201 '<|reserved_special_token_193|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128074 '<|reserved_special_token_66|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128095 '<|reserved_special_token_87|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128186 '<|reserved_special_token_178|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128143 '<|reserved_special_token_135|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128229 '<|reserved_special_token_221|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128007 '<|end_header_id|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128055 '<|reserved_special_token_47|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128056 '<|reserved_special_token_48|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128061 '<|reserved_special_token_53|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128153 '<|reserved_special_token_145|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128152 '<|reserved_special_token_144|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128212 '<|reserved_special_token_204|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128172 '<|reserved_special_token_164|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128160 '<|reserved_special_token_152|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128041 '<|reserved_special_token_33|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128181 '<|reserved_special_token_173|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128094 '<|reserved_special_token_86|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128118 '<|reserved_special_token_110|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128236 '<|reserved_special_token_228|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128148 '<|reserved_special_token_140|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128042 '<|reserved_special_token_34|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128139 '<|reserved_special_token_131|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128173 '<|reserved_special_token_165|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128239 '<|reserved_special_token_231|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128157 '<|reserved_special_token_149|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128052 '<|reserved_special_token_44|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128026 '<|reserved_special_token_18|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128003 '<|reserved_special_token_1|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128019 '<|reserved_special_token_11|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128116 '<|reserved_special_token_108|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128161 '<|reserved_special_token_153|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128000 '<｜begin▁of▁sentence｜>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128226 '<|reserved_special_token_218|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128159 '<|reserved_special_token_151|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128088 '<|reserved_special_token_80|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128163 '<|reserved_special_token_155|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128113 '<|reserved_special_token_105|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128250 '<|reserved_special_token_242|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128125 '<|reserved_special_token_117|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128053 '<|reserved_special_token_45|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128224 '<|reserved_special_token_216|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128247 '<|reserved_special_token_239|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128251 '<|reserved_special_token_243|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128216 '<|reserved_special_token_208|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128006 '<|start_header_id|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128211 '<|reserved_special_token_203|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128077 '<|reserved_special_token_69|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128237 '<|reserved_special_token_229|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128086 '<|reserved_special_token_78|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128001 '<｜end▁of▁sentence｜>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128227 '<|reserved_special_token_219|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128058 '<|reserved_special_token_50|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128100 '<|reserved_special_token_92|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128209 '<|reserved_special_token_201|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128084 '<|reserved_special_token_76|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128071 '<|reserved_special_token_63|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128070 '<|reserved_special_token_62|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128049 '<|reserved_special_token_41|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128197 '<|reserved_special_token_189|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128072 '<|reserved_special_token_64|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128223 '<|reserved_special_token_215|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128217 '<|reserved_special_token_209|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128111 '<|reserved_special_token_103|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128203 '<|reserved_special_token_195|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128051 '<|reserved_special_token_43|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128030 '<|reserved_special_token_22|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128117 '<|reserved_special_token_109|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128010 '<|python_tag|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128238 '<|reserved_special_token_230|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128255 '<|reserved_special_token_247|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128202 '<|reserved_special_token_194|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128132 '<|reserved_special_token_124|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128248 '<|reserved_special_token_240|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128167 '<|reserved_special_token_159|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128127 '<|reserved_special_token_119|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128105 '<|reserved_special_token_97|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128039 '<|reserved_special_token_31|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128232 '<|reserved_special_token_224|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128166 '<|reserved_special_token_158|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128130 '<|reserved_special_token_122|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128114 '<|reserved_special_token_106|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128234 '<|reserved_special_token_226|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128191 '<|reserved_special_token_183|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128064 '<|reserved_special_token_56|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128140 '<|reserved_special_token_132|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128096 '<|reserved_special_token_88|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128098 '<|reserved_special_token_90|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128192 '<|reserved_special_token_184|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128093 '<|reserved_special_token_85|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128150 '<|reserved_special_token_142|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128222 '<|reserved_special_token_214|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128233 '<|reserved_special_token_225|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128220 '<|reserved_special_token_212|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128034 '<|reserved_special_token_26|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128033 '<|reserved_special_token_25|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128253 '<|reserved_special_token_245|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128195 '<|reserved_special_token_187|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128099 '<|reserved_special_token_91|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128189 '<|reserved_special_token_181|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128210 '<|reserved_special_token_202|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128174 '<|reserved_special_token_166|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128083 '<|reserved_special_token_75|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128080 '<|reserved_special_token_72|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128104 '<|reserved_special_token_96|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128082 '<|reserved_special_token_74|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128219 '<|reserved_special_token_211|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128017 '<|reserved_special_token_9|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128050 '<|reserved_special_token_42|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128205 '<|reserved_special_token_197|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128047 '<|reserved_special_token_39|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128164 '<|reserved_special_token_156|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128020 '<|reserved_special_token_12|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128069 '<|reserved_special_token_61|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128245 '<|reserved_special_token_237|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128121 '<|reserved_special_token_113|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128079 '<|reserved_special_token_71|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128037 '<|reserved_special_token_29|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128244 '<|reserved_special_token_236|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128029 '<|reserved_special_token_21|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128221 '<|reserved_special_token_213|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128066 '<|reserved_special_token_58|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128120 '<|reserved_special_token_112|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128015 '<｜▁pad▁｜>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128025 '<|reserved_special_token_17|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128126 '<|reserved_special_token_118|>' is not marked as EOG\n",
            "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "llm_load_vocab: special tokens cache size = 256\n",
            "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 128256\n",
            "llm_load_print_meta: n_merges         = 280147\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 131072\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 500000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
            "llm_load_print_meta: model type       = 8B\n",
            "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
            "llm_load_print_meta: model params     = 8.03 B\n",
            "llm_load_print_meta: model size       = 5.33 GiB (5.70 BPW) \n",
            "llm_load_print_meta: general.name     = DeepSeek R1 Distill Llama 8B\n",
            "llm_load_print_meta: BOS token        = 128000 '<｜begin▁of▁sentence｜>'\n",
            "llm_load_print_meta: EOS token        = 128001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'\n",
            "llm_load_print_meta: PAD token        = 128001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: LF token         = 128 'Ä'\n",
            "llm_load_print_meta: EOG token        = 128001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'\n",
            "llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: max token length = 256\n",
            "llm_load_tensors: tensor 'token_embd.weight' (q5_K) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
            "llm_load_tensors: offloading 32 repeating layers to GPU\n",
            "llm_load_tensors: offloading output layer to GPU\n",
            "llm_load_tensors: offloaded 33/33 layers to GPU\n",
            "llm_load_tensors:        CUDA0 model buffer size =  5115.49 MiB\n",
            "llm_load_tensors:   CPU_Mapped model buffer size =   344.44 MiB\n",
            "........................................................................................\n",
            "llama_new_context_with_model: n_seq_max     = 1\n",
            "llama_new_context_with_model: n_ctx         = 16384\n",
            "llama_new_context_with_model: n_ctx_per_seq = 16384\n",
            "llama_new_context_with_model: n_batch       = 512\n",
            "llama_new_context_with_model: n_ubatch      = 512\n",
            "llama_new_context_with_model: flash_attn    = 0\n",
            "llama_new_context_with_model: freq_base     = 500000.0\n",
            "llama_new_context_with_model: freq_scale    = 1\n",
            "llama_new_context_with_model: n_ctx_per_seq (16384) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_kv_cache_init: kv_size = 16384, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
            "llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 28: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 29: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 30: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 31: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =  2048.00 MiB\n",
            "llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.49 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =  1088.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =    40.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 2\n",
            "CUDA : ARCHS = 500,520,530,600,610,620,700,720,750,800,860,870,890,900 | FORCE_MMQ = 1 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
            "Model metadata: {'quantize.imatrix.entries_count': '224', 'quantize.imatrix.dataset': '/training_dir/calibration_datav3.txt', 'quantize.imatrix.chunks_count': '125', 'quantize.imatrix.file': '/models_out/DeepSeek-R1-Distill-Llama-8B-GGUF/DeepSeek-R1-Distill-Llama-8B.imatrix', 'general.file_type': '17', 'tokenizer.chat_template': \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='') %}{%- for message in messages %}{%- if message['role'] == 'system' %}{% set ns.system_prompt = message['content'] %}{%- endif %}{%- endfor %}{{bos_token}}{{ns.system_prompt}}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{{'<｜User｜>' + message['content']}}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is none %}{%- set ns.is_tool = false -%}{%- for tool in message['tool_calls']%}{%- if not ns.is_first %}{{'<｜Assistant｜><｜tool▁calls▁begin｜><｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\\\n' + '```json' + '\\\\n' + tool['function']['arguments'] + '\\\\n' + '```' + '<｜tool▁call▁end｜>'}}{%- set ns.is_first = true -%}{%- else %}{{'\\\\n' + '<｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\\\n' + '```json' + '\\\\n' + tool['function']['arguments'] + '\\\\n' + '```' + '<｜tool▁call▁end｜>'}}{{'<｜tool▁calls▁end｜><｜end▁of▁sentence｜>'}}{%- endif %}{%- endfor %}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is not none %}{%- if ns.is_tool %}{{'<｜tool▁outputs▁end｜>' + message['content'] + '<｜end▁of▁sentence｜>'}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message['content'] %}{% if '</think>' in content %}{% set content = content.split('</think>')[-1] %}{% endif %}{{'<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<｜tool▁outputs▁begin｜><｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- set ns.is_output_first = false %}{%- else %}{{'\\\\n<｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<｜tool▁outputs▁end｜>'}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{'<｜Assistant｜>'}}{% endif %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.eos_token_id': '128001', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'llama.rope.dimension_count': '128', 'llama.vocab_size': '128256', 'general.architecture': 'llama', 'llama.rope.freq_base': '500000.000000', 'tokenizer.ggml.padding_token_id': '128001', 'general.basename': 'DeepSeek-R1-Distill-Llama', 'tokenizer.ggml.bos_token_id': '128000', 'llama.attention.head_count': '32', 'tokenizer.ggml.pre': 'llama-bpe', 'llama.context_length': '131072', 'general.name': 'DeepSeek R1 Distill Llama 8B', 'general.type': 'model', 'general.size_label': '8B', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Using gguf chat template: {% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='') %}{%- for message in messages %}{%- if message['role'] == 'system' %}{% set ns.system_prompt = message['content'] %}{%- endif %}{%- endfor %}{{bos_token}}{{ns.system_prompt}}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{{'<｜User｜>' + message['content']}}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is none %}{%- set ns.is_tool = false -%}{%- for tool in message['tool_calls']%}{%- if not ns.is_first %}{{'<｜Assistant｜><｜tool▁calls▁begin｜><｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{%- set ns.is_first = true -%}{%- else %}{{'\\n' + '<｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{{'<｜tool▁calls▁end｜><｜end▁of▁sentence｜>'}}{%- endif %}{%- endfor %}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is not none %}{%- if ns.is_tool %}{{'<｜tool▁outputs▁end｜>' + message['content'] + '<｜end▁of▁sentence｜>'}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message['content'] %}{% if '</think>' in content %}{% set content = content.split('</think>')[-1] %}{% endif %}{{'<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<｜tool▁outputs▁begin｜><｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- set ns.is_output_first = false %}{%- else %}{{'\\n<｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<｜tool▁outputs▁end｜>'}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{'<｜Assistant｜>'}}{% endif %}\n",
            "Using chat eos_token: <｜end▁of▁sentence｜>\n",
            "Using chat bos_token: <｜begin▁of▁sentence｜>\n",
            "\u001b[2;36m17:07:17-890975\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loaded \u001b[32m\"DeepSeek-R1-Distill-Llama-8B-Q5_K_M.gguf\"\u001b[0m in   \n",
            "\u001b[2;36m                \u001b[0m         \u001b[1;36m3.71\u001b[0m seconds.                                          \n",
            "\u001b[2;36m17:07:17-892132\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m LOADER: \u001b[32m\"llama.cpp\"\u001b[0m                                    \n",
            "\u001b[2;36m17:07:17-892919\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m TRUNCATION LENGTH: \u001b[1;36m16384\u001b[0m                               \n",
            "\u001b[2;36m17:07:17-893675\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m INSTRUCTION TEMPLATE: \u001b[32m\"Custom \u001b[0m\u001b[32m(\u001b[0m\u001b[32mobtained from model \u001b[0m    \n",
            "\u001b[2;36m                \u001b[0m         \u001b[32mmetadata\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\"\u001b[0m                                             \n",
            "\n",
            "Running on local URL:  http://127.0.0.1:7860\n",
            "\n",
            "Running on public URL: https://90c38b7c935cfc8a9a.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/queueing.py\", line 541, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/route_utils.py\", line 276, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1928, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1514, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/utils.py\", line 833, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/text-generation-webui/modules/chat.py\", line 1160, in handle_character_menu_change\n",
            "    name1, name2, picture, greeting, context = load_character(state['character_menu'], state['name1'], state['name2'])\n",
            "                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/text-generation-webui/modules/chat.py\", line 754, in load_character\n",
            "    data = json.loads(file_contents) if extension == \"json\" else yaml.safe_load(file_contents)\n",
            "                                                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/yaml/__init__.py\", line 125, in safe_load\n",
            "    return load(stream, SafeLoader)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/yaml/__init__.py\", line 81, in load\n",
            "    return loader.get_single_data()\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/yaml/constructor.py\", line 49, in get_single_data\n",
            "    node = self.get_single_node()\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/yaml/composer.py\", line 36, in get_single_node\n",
            "    document = self.compose_document()\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/yaml/composer.py\", line 55, in compose_document\n",
            "    node = self.compose_node(None, None)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/yaml/composer.py\", line 84, in compose_node\n",
            "    node = self.compose_mapping_node(anchor)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/yaml/composer.py\", line 127, in compose_mapping_node\n",
            "    while not self.check_event(MappingEndEvent):\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/yaml/parser.py\", line 98, in check_event\n",
            "    self.current_event = self.state()\n",
            "                         ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/yaml/parser.py\", line 428, in parse_block_mapping_key\n",
            "    if self.check_token(KeyToken):\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/yaml/scanner.py\", line 115, in check_token\n",
            "    while self.need_more_tokens():\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/yaml/scanner.py\", line 152, in need_more_tokens\n",
            "    self.stale_possible_simple_keys()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/yaml/scanner.py\", line 291, in stale_possible_simple_keys\n",
            "    raise ScannerError(\"while scanning a simple key\", key.mark,\n",
            "yaml.scanner.ScannerError: while scanning a simple key\n",
            "  in \"<unicode string>\", line 5, column 1:\n",
            "    You are an AI specialized in gen ... \n",
            "    ^\n",
            "could not find expected ':'\n",
            "  in \"<unicode string>\", line 6, column 1:\n",
            "    High-Precision Electronic Circui ... \n",
            "    ^\n",
            "/usr/local/lib/python3.11/dist-packages/llama_cpp_cuda/llama.py:1237: RuntimeWarning: Detected duplicate leading \"<｜begin▁of▁sentence｜>\" in prompt, this will likely reduce response quality, consider removing it...\n",
            "  warnings.warn(\n",
            "llama_perf_context_print:        load time =     584.24 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    82 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /   363 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =   16237.17 ms /   445 tokens\n",
            "Output generated in 16.25 seconds (22.34 tokens/s, 363 tokens, context 99, seed 1640037781)\n",
            "Llama.generate: 1 prefix-match hit, remaining 62 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     584.24 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /   511 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =   22120.68 ms /   573 tokens\n",
            "Output generated in 22.13 seconds (23.13 tokens/s, 512 tokens, context 63, seed 1728208422)\n",
            "Llama.generate: 574 prefix-match hit, remaining 11 prompt tokens to eval\n",
            "Output generated in 6.99 seconds (24.03 tokens/s, 168 tokens, context 585, seed 826163378)\n"
          ]
        }
      ],
      "source": [
        "%cd /content\n",
        "!apt-get -y install -qq aria2\n",
        "!pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "!git clone -b V20250121 https://github.com/Troyanovsky/text-generation-webui\n",
        "%cd /content/text-generation-webui\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/bartowski/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q5_K_M.gguf?download=true -d /content/text-generation-webui/models/ -o DeepSeek-R1-Distill-Llama-8B-Q5_K_M.gguf\n",
        "%cd /content/text-generation-webui\n",
        "!python server.py --share --n-gpu-layers 100000 --model DeepSeek-R1-Distill-Llama-8B-Q5_K_M.gguf --n_ctx 16384"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Restart the web UI:"
      ],
      "metadata": {
        "id": "-M4P-0Cw_6NH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python server.py --share --n-gpu-layers 100000 --model DeepSeek-R1-Distill-Llama-8B-Q5_K_M.gguf --n_ctx 16384"
      ],
      "metadata": {
        "id": "6nuotdRn_y_h"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}